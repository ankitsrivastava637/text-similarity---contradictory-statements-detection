{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-similarity and contradictory statements.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPz3QryBL6PxQrlPy+U/sR+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitsrivastava637/text-similarity---contradictory-statements-detection/blob/main/text_similarity_and_contradictory_statements.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjtW9wJ-WWf9",
        "outputId": "aff4d004-9884-4b02-8ed7-12c39d738356"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "start = time.time()\n",
        "# Single list of sentences - Possible tens of thousands of sentences\n",
        "sentences = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'I love pasta',\n",
        "             'The new movie is awesome',\n",
        "             'The cat plays in the garden',\n",
        "             'A woman watches TV',\n",
        "             'The new movie is so great',\n",
        "             'Do you like pizza?']\n",
        "\n",
        "paraphrases = util.paraphrase_mining(model, sentences)\n",
        "\n",
        "for paraphrase in paraphrases[0:10]:\n",
        "       score, i, j = paraphrase\n",
        "       if sentences[i] == 'The cat sits outside' and score>0.70:\n",
        "           print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))\n",
        "           print\n",
        "           break\n",
        "       else:\n",
        "           print('Request handover')  \n",
        "           break  \n",
        "end = time.time()    \n",
        "print(end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Request handover\n",
            "0.11644530296325684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8i-rk6xXK4Z",
        "outputId": "49681896-a8c4-41ca-9ef5-48821bfeb392"
      },
      "source": [
        "#Explacy\n",
        "\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "_do_print_debug_info = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Taking the sentences as input\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "sent1=\"\"\"We are a finance company. \n",
        "          I like Python because I can build applications,\n",
        "          We are trying to give best service.\n",
        "          The cat sits on the ground.\n",
        "          We do not accept credit cards. \n",
        "         The cat walks on the sidewalk\"\"\"\n",
        "sent2=\"We only accept credit cards.\"\n",
        "\n",
        "#print(\"\\n\")\n",
        "\n",
        "start = time.time()\n",
        "doc1 = nlp(sent1)\n",
        "doc2 = nlp(sent2)\n",
        "\n",
        "\n",
        "#Antonyms finder function\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "#word=\"went\"\n",
        "def antysyn(word):\n",
        "    for syn in wordnet.synsets(word):\n",
        "        #print(syn)\n",
        "        for l in syn.lemmas():\n",
        "            #print(l)\n",
        "            synonyms.append(l.name())\n",
        "            if l.antonyms():\n",
        "                antonyms.append(l.antonyms()[0].name())\n",
        "    #print(\"Synonym:\",set(synonyms))\n",
        "    #print(\"Antonym:\",set(antonyms))   \n",
        "\n",
        "\n",
        "#Initializing required variables and lists.\n",
        "wrdlist=list()\n",
        "antony=list()\n",
        "contr_tracker=0\n",
        "antonym_tracker=0\n",
        "\n",
        "negdoc1=0\n",
        "negdoc2=0\n",
        "verb1=\"\"\n",
        "verb2=\"\"\n",
        "num_contr_tracker=0\n",
        "\n",
        "\n",
        "#Processing sentence 1\n",
        "for token in doc1:\n",
        "    if(token.dep_==\"neg\"):\n",
        "        negdoc1=1\n",
        "        verb1+=\"NOT \"\n",
        "    #Storing the antonyms of root words\n",
        "    if(token.pos_==\"VERB\" and token.dep_==\"ROOT\"):\n",
        "        #print(token.text)\n",
        "        verb1+=token.lemma_\n",
        "        antysyn(token.lemma_)\n",
        "        for anton in antonyms:\n",
        "            antony.append(anton)\n",
        "\n",
        "\n",
        "#Processing Sentence 2\n",
        "for token in doc2:\n",
        "    if(token.dep_==\"neg\"):\n",
        "        negdoc2=1\n",
        "        verb2+=\"NOT \"\n",
        "    if(token.pos_==\"VERB\" and token.dep_==\"ROOT\"):\n",
        "        verb2+=token.lemma_\n",
        "        if(token.lemma_ in antony):\n",
        "            antonym_tracker=1\n",
        "\n",
        "\n",
        "\n",
        "#Function for checking negation\n",
        "def checknegationcontradiction(antonym_tracker,negdoc1,negdoc2):\n",
        "    temp_var=negdoc1+negdoc2+antonym_tracker\n",
        "    if(temp_var%2!=0 and temp_var<3):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "#Checking contradiction due to negation\n",
        "contr_tracker=checknegationcontradiction(antonym_tracker,negdoc1,negdoc2)\n",
        "\n",
        "\n",
        "def check_words(doc):\n",
        "    merged_word=\"\"\n",
        "    #tokens = word_tokenize(doc)\n",
        "    WordNum  = len(doc)\n",
        "    #print(\"WordNum is:\"+str(WordNum))\n",
        "    for i in range(WordNum):\n",
        "        #print(i,doc[i],doc[i].ent_type_)\n",
        "        #print(str.isdigit(doc[i].text))\n",
        "        if(doc[i].ent_type_==\"CARDINAL\" or doc[i].pos_==\"NUM\"):\n",
        "             merged_word = doc[i].text\n",
        "        if((doc[i].ent_type_==\"CARDINAL\" or doc[i].pos_==\"NUM\") and str.isdigit(doc[i].text)):\n",
        "            if(not(str.isdigit(doc[i-1].text))):\n",
        "                if(not(str.isdigit(doc[i-2].text))):\n",
        "                    merged_word= doc[i-2].text+' '+doc[i-1].text+' '+doc[i].text\n",
        "    return merged_word\n",
        "\n",
        "\n",
        "def check_values(t1,t2):\n",
        "    for phrase in checklist_more:\n",
        "        if(t1.find(phrase)!=-1 and t2.find(phrase)==-1):\n",
        "            num1 = t1.replace(phrase,'')\n",
        "            num2 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "            num2=num2[0]\n",
        "            if int(num1)>num2:\n",
        "                print(\"case1\")\n",
        "                return('Contradiction')\n",
        "            else:\n",
        "                return(\"No Contradiction\")\n",
        "        else:\n",
        "            return(\"No Contradiction\")\n",
        "    for phrase in checklist_more:\n",
        "        if(t2.find(phrase)!=-1 and t1.find(phrase)==-1):\n",
        "            num2 = t2.replace(phrase,'')\n",
        "            num1 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "            num1=num1[0]\n",
        "            if int(num2)>num1:\n",
        "                print(\"case2\")\n",
        "                return('Contradiction') \n",
        "            else:\n",
        "                return(\"No Contradiction\")\n",
        "        else:\n",
        "            return(\"No Contradiction\")\n",
        "    for phrase in checklist_less:\n",
        "        if(t1.find(phrase)!=-1 and t2.find(phrase)==-1):\n",
        "            num1 = t1.replace(phrase,'')\n",
        "            num2 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "            num2=num2[0]\n",
        "            if int(num1)<num2:\n",
        "                print(\"case3\")\n",
        "                return('Contradiction')\n",
        "            else:\n",
        "                return(\"No Contradiction\")\n",
        "        else:\n",
        "                return(\"No Contradiction\")\n",
        "    for phrase in checklist_less:\n",
        "        if(t2.find(phrase)!=-1 and t1.find(phrase)==-1):\n",
        "            num2 = t2.replace(phrase,'')\n",
        "            num1 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "            num1=num1[0]\n",
        "            if int(num1)>num2:\n",
        "                print(\"case4\")\n",
        "                return('Contradiction')\n",
        "            else: \n",
        "                return(\"No Contradiction\")\n",
        "        else:\n",
        "                return(\"No Contradiction\")\n",
        "    else:\n",
        "        if(t1!=t2):\n",
        "            return('Contradiction') \n",
        "\n",
        "#Finding numerical mismatch\n",
        "checklist_more=['more than ', 'greater than ', 'above', 'all']\n",
        "checklist_less=['less than ', 'lesser than ', 'below', 'only']\n",
        "\n",
        "\n",
        "x1 = check_words(doc1)\n",
        "#print(x1)\n",
        "y1 = check_words(doc2)\n",
        "#print(\"Second Sentence:\",y1)\n",
        "number_contr_tracker=check_values(x1,y1)\n",
        "if(number_contr_tracker=='Contradiction'):\n",
        "    num_contr_tracker=1\n",
        "else:\n",
        "    num_contr_tracker=0\n",
        "\n",
        "\n",
        "if contr_tracker==1:\n",
        "    print(\"\\n\",\"->\",verb1.upper(),\"and\",verb2.upper(),\"can't happen simultaneously.\")\n",
        "    print(\"->Antonymity/Negation contradiction FOUND.\")\n",
        "else:\n",
        "    print(\"\\n->Antonymity/Negation contradiction NOT found.\")\n",
        "\n",
        "if num_contr_tracker==1:\n",
        "    print(\"->Numeric Mismatch Contradiction FOUND.\")\n",
        "else:\n",
        "    print(\"->Numeric Mismatch Contradiction NOT Found.\")    \n",
        "\n",
        "end = time.time()\n",
        "print(\"total time taken:\", end-start)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "\n",
            " -> LIKETRYSITNOT ACCEPTWALK and ACCEPT can't happen simultaneously.\n",
            "->Antonymity/Negation contradiction FOUND.\n",
            "->Numeric Mismatch Contradiction NOT Found.\n",
            "total time taken: 0.14984536170959473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sJ4PAqQJdf5",
        "outputId": "1f00d271-6729-499a-9882-8306309437f7"
      },
      "source": [
        "import time\n",
        "#Explacy\n",
        "\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "_do_print_debug_info = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Taking the sentences as input\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ContradictionFilter:\n",
        "    def __init__(self, context, reply):\n",
        "        self.context = context\n",
        "        self.reply = reply\n",
        "\n",
        "\n",
        "    def contradiction_detection(self):\n",
        "\n",
        "        sent1=self.context\n",
        "        sent2=self.reply\n",
        "        \n",
        "    #print(\"\\n\")\n",
        "\n",
        "        start = time.time()\n",
        "        doc1 = nlp(sent1)\n",
        "        doc2 = nlp(sent2)\n",
        "        \n",
        "        \n",
        "        #Antonyms finder function\n",
        "        synonyms = []\n",
        "        antonyms = []\n",
        "        #word=\"went\"\n",
        "        def antysyn(word):\n",
        "            for syn in wordnet.synsets(word):\n",
        "                #print(syn)\n",
        "                for l in syn.lemmas():\n",
        "                    #print(l)\n",
        "                    synonyms.append(l.name())\n",
        "                    if l.antonyms():\n",
        "                        antonyms.append(l.antonyms()[0].name())\n",
        "            #print(\"Synonym:\",set(synonyms))\n",
        "            #print(\"Antonym:\",set(antonyms))   \n",
        "        \n",
        "        \n",
        "        #Initializing required variables and lists.\n",
        "        wrdlist=list()\n",
        "        antony=list()\n",
        "        contr_tracker=0\n",
        "        antonym_tracker=0\n",
        "        \n",
        "        negdoc1=0\n",
        "        negdoc2=0\n",
        "        verb1=\"\"\n",
        "        verb2=\"\"\n",
        "        num_contr_tracker=0\n",
        "        \n",
        "        \n",
        "        #Processing sentence 1\n",
        "        for token in doc1:\n",
        "            if(token.dep_==\"neg\"):\n",
        "                negdoc1=1\n",
        "                verb1+=\"NOT \"\n",
        "            #Storing the antonyms of root words\n",
        "            if(token.pos_==\"VERB\" and token.dep_==\"ROOT\"):\n",
        "                #print(token.text)\n",
        "                verb1+=token.lemma_\n",
        "                antysyn(token.lemma_)\n",
        "                for anton in antonyms:\n",
        "                    antony.append(anton)\n",
        "        \n",
        "        \n",
        "        #Processing Sentence 2\n",
        "        for token in doc2:\n",
        "            if(token.dep_==\"neg\"):\n",
        "                negdoc2=1\n",
        "                verb2+=\"NOT \"\n",
        "            if(token.pos_==\"VERB\" and token.dep_==\"ROOT\"):\n",
        "                verb2+=token.lemma_\n",
        "                if(token.lemma_ in antony):\n",
        "                    antonym_tracker=1\n",
        "        \n",
        "        \n",
        "        \n",
        "        #Function for checking negation\n",
        "        def checknegationcontradiction(antonym_tracker,negdoc1,negdoc2):\n",
        "            temp_var=negdoc1+negdoc2+antonym_tracker\n",
        "            if(temp_var%2!=0 and temp_var<3):\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "        \n",
        "        #Checking contradiction due to negation\n",
        "        contr_tracker=checknegationcontradiction(antonym_tracker,negdoc1,negdoc2)\n",
        "        \n",
        "        \n",
        "        def check_words(doc):\n",
        "            merged_word=\"\"\n",
        "            #tokens = word_tokenize(doc)\n",
        "            WordNum  = len(doc)\n",
        "            #print(\"WordNum is:\"+str(WordNum))\n",
        "            for i in range(WordNum):\n",
        "                #print(i,doc[i],doc[i].ent_type_)\n",
        "                #print(str.isdigit(doc[i].text))\n",
        "                if(doc[i].ent_type_==\"CARDINAL\" or doc[i].pos_==\"NUM\"):\n",
        "                    merged_word = doc[i].text\n",
        "                if((doc[i].ent_type_==\"CARDINAL\" or doc[i].pos_==\"NUM\") and str.isdigit(doc[i].text)):\n",
        "                    if(not(str.isdigit(doc[i-1].text))):\n",
        "                        if(not(str.isdigit(doc[i-2].text))):\n",
        "                            merged_word= doc[i-2].text+' '+doc[i-1].text+' '+doc[i].text\n",
        "            return merged_word\n",
        "        \n",
        "        \n",
        "        def check_values(t1,t2):\n",
        "            for phrase in checklist_more:\n",
        "                if(t1.find(phrase)!=-1 and t2.find(phrase)==-1):\n",
        "                    num1 = t1.replace(phrase,'')\n",
        "                    num2 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "                    num2=num2[0]\n",
        "                    if int(num1)>num2:\n",
        "                        print(\"case1\")\n",
        "                        return('Contradiction')\n",
        "                    else:\n",
        "                        return(\"No Contradiction\")\n",
        "                else:\n",
        "                    return(\"No Contradiction\")\n",
        "            for phrase in checklist_more:\n",
        "                if(t2.find(phrase)!=-1 and t1.find(phrase)==-1):\n",
        "                    num2 = t2.replace(phrase,'')\n",
        "                    num1 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "                    num1=num1[0]\n",
        "                    if int(num2)>num1:\n",
        "                        print(\"case2\")\n",
        "                        return('Contradiction') \n",
        "                    else:\n",
        "                        return(\"No Contradiction\")\n",
        "                else:\n",
        "                    return(\"No Contradiction\")\n",
        "            for phrase in checklist_less:\n",
        "                if(t1.find(phrase)!=-1 and t2.find(phrase)==-1):\n",
        "                    num1 = t1.replace(phrase,'')\n",
        "                    num2 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "                    num2=num2[0]\n",
        "                    if int(num1)<num2:\n",
        "                        print(\"case3\")\n",
        "                        return('Contradiction')\n",
        "                    else:\n",
        "                        return(\"No Contradiction\")\n",
        "                else:\n",
        "                        return(\"No Contradiction\")\n",
        "            for phrase in checklist_less:\n",
        "                if(t2.find(phrase)!=-1 and t1.find(phrase)==-1):\n",
        "                    num2 = t2.replace(phrase,'')\n",
        "                    num1 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "                    num1=num1[0]\n",
        "                    if int(num1)>num2:\n",
        "                        print(\"case4\")\n",
        "                        return('Contradiction')\n",
        "                    else: \n",
        "                        return(\"No Contradiction\")\n",
        "                else:\n",
        "                        return(\"No Contradiction\")\n",
        "            else:\n",
        "                if(t1!=t2):\n",
        "                    return('Contradiction') \n",
        "        \n",
        "        #Finding numerical mismatch\n",
        "        checklist_more=['more than ', 'greater than ', 'above', 'all', 'all']\n",
        "        checklist_less=['less than ', 'lesser than ', 'below', 'only', 'few']\n",
        "        \n",
        "        \n",
        "        x1 = check_words(doc1)\n",
        "        #print(x1)\n",
        "        y1 = check_words(doc2)\n",
        "        #print(\"Second Sentence:\",y1)\n",
        "        number_contr_tracker=check_values(x1,y1)\n",
        "        if(number_contr_tracker=='Contradiction'):\n",
        "            num_contr_tracker=1\n",
        "        else:\n",
        "            num_contr_tracker=0\n",
        "        \n",
        "        if contr_tracker==0 and num_contr_tracker==0:\n",
        "            return self.reply\n",
        "        \n",
        "        if contr_tracker==1 or num_contr_tracker==1:\n",
        "            #print(\"\\n\",\"->\",verb1.upper(),\"and\",verb2.upper(),\"can't happen simultaneously.\")\n",
        "            #print(\"->Antonymity/Negation contradiction FOUND.\")\n",
        "            return 'Reuest Handover'\n",
        "        else:\n",
        "            print(\"\\n->Antonymity/Negation contradiction NOT found.\")\n",
        "            return self.reply\n",
        "        \n",
        "        if num_contr_tracker==1:\n",
        "            print(\"->Numeric Mismatch Contradiction FOUND.\")\n",
        "            #return 'Request Handover'\n",
        "        else:\n",
        "            print(\"->Numeric Mismatch Contradiction NOT Found.\")    \n",
        "            #return self.reply\n",
        "        end = time.time()\n",
        "\n",
        "        print(\"total time taken:\", end-start) \n",
        "           "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj9Q9g2jKdwt",
        "outputId": "866f0e0e-5ffe-411d-8105-bcfbc948df6b"
      },
      "source": [
        "context = \"\"\"We are a finance company. \n",
        "             I like Python because I can build applications,\n",
        "             We are trying to give best service.\n",
        "             The cat sits on the ground.\n",
        "             We do not accept credit cards. \n",
        "             The cat walks on the sidewalk\"\"\"\n",
        "cont = \"We accept all credit cards.\"\n",
        "reply = \"We don't accept cash.\"\n",
        "text_infer_obj = ContradictionFilter(context, reply)\n",
        "text_infer = text_infer_obj.contradiction_detection()   #gives entailment, contradiction, neutral scores between context and response\n",
        "print(text_infer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We don't accept cash.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDXH6DmNfu24",
        "outputId": "ecf64d19-cc37-48ef-ac56-b17b53ed2549"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentence_transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 29.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 32.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Collecting sentence_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/75/df441011cd1726822b70fbff50042adb4860e9327b99b346154ead704c44/sentence-transformers-1.2.0.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.0.45)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.2.0-cp37-none-any.whl size=123339 sha256=9185c6786d9b4ec294fdf96603efe2bfc96310568102aec30eec2e69f6a72143\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/06/f7/faaa96fdda87462b4fd5c47b343340e9d5531ef70d0eef8242\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-1.2.0 sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0K23USVhyfQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS2V3y7Nhymw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLz4eVRFhyp6",
        "outputId": "bf37b26d-e0e3-4a6b-9a36-54956176e592"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "import time\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "_do_print_debug_info = False\n",
        "\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "nlp =  spacy.load(\"en_core_web_md\")\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "class ResponseFilter:\n",
        "    def __init__(self, context, reply):\n",
        "        self.context = context\n",
        "        self.reply = reply\n",
        "\n",
        "    def textual_similarity(self):\n",
        "         # Single list of sentences - Possible tens of thousands of sentences\n",
        "        context_reply = (self.context + ' ' + self.reply)\n",
        "        sent_list = nltk.tokenize.sent_tokenize(context_reply)\n",
        "        sentences = sent_list\n",
        "        #print(sentences)\n",
        "        index_of_reply = len(self.reply)\n",
        "\n",
        "        paraphrases = util.paraphrase_mining(model, sentences)\n",
        "        #print(paraphrases)\n",
        "        lst = []\n",
        "\n",
        "        for paraphrase in paraphrases[0:10]:\n",
        "           score, i, j = paraphrase\n",
        "           #print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))\n",
        "           #print('parAPHRASE:',paraphrase)\n",
        "           #print(i)\n",
        "          #print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))\n",
        "           lst.append([sentences[i], sentences[j], score])\n",
        "        #print(lst)\n",
        "          \n",
        "        for ls in lst:\n",
        "          #print(ls[0])\n",
        "          #print(sentences[len(sentences)-1])\n",
        "          if ls[0] == sentences[len(sentences)-1] or ls[1] == sentences[len(sentences)-1]:\n",
        "            #print(sentences[len(sentences)-1])\n",
        "            #print(ls[0])\n",
        "            if  ls[2] > 0.65:\n",
        "                text_sim_obj = ContradictionFilter(self.context, self.reply)\n",
        "                text_sim = text_sim_obj.contradiction_detection()   #gives entailment, contradiction, neutral scores between context and response\n",
        "                return text_sim\n",
        "            else:\n",
        "              return 'Request Handover'\n",
        "          else:\n",
        "            continue        \n",
        "    \n",
        "\n",
        "end = time.time()    \n",
        "print(end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "0.0002956390380859375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GQf1eobzM8i",
        "outputId": "67b0f603-5e6c-47a6-db98-e5ec3f4296fc"
      },
      "source": [
        "start = time.time()\n",
        "context = \"\"\"We are a finance company. \n",
        "             I like Python because I can build applications.\n",
        "             We are trying to give best service.\n",
        "             We do accept credit cards. \n",
        "             The cat walks on the sidewalk.\n",
        "             Credit cards are Capital One’s main physical product, and teams all over the company work to support this product whether it’s in the form of product development, software engineering, design, etc. For example, Alana is a data engineer working on rewards applications for external partners while Sahana works to support data scientists developing machine learning models used to derive insights from card customer calls. Like many others at Capital One, we know what needs to be done to support credit card initiatives, but we were curious about how physical credit cards actually work!\n",
        "             \"\"\"\n",
        "cont = \"We accept credit card.\"\n",
        "reply = \"We accept credit cards.\" \n",
        "print(len(context.split(' ')))     \n",
        "text_infer_obj = ResponseFilter(context, reply)   \n",
        "text_infer = text_infer_obj.textual_similarity()   #gives entailment, contradiction, neutral scores between context and response\n",
        "print(text_infer)\n",
        "end = time.time()\n",
        "print('time taken:', end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "199\n",
            "We accept credit cards.\n",
            "time taken: 0.05808424949645996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oboFeFnCy8ao"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLmTR3NOy8jD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svukQCe0y8mS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2032e19-5a8e-42af-ca9d-4442ce228895"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "import time\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "_do_print_debug_info = False\n",
        "\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "nlp =  spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "class ResponseFilter:\n",
        "    def __init__(self, context, reply):\n",
        "        self.context = context\n",
        "        self.reply = reply\n",
        "\n",
        "    def textual_similarity(self):\n",
        "         # Single list of sentences - Possible tens of thousands of sentences\n",
        "        context_reply = (self.context + ' ' + self.reply)\n",
        "        sent_list = nltk.tokenize.sent_tokenize(context_reply)\n",
        "        sentences = sent_list\n",
        "        #print(sentences)\n",
        "        index_of_reply = len(self.reply)\n",
        "\n",
        "        paraphrases = util.paraphrase_mining(model, sentences)\n",
        "        #print(paraphrases)\n",
        "        lst = []\n",
        "\n",
        "        for paraphrase in paraphrases[0:10]:\n",
        "           score, i, j = paraphrase\n",
        "           print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))\n",
        "           #print('parAPHRASE:',paraphrase)\n",
        "           #print(i)\n",
        "          #print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))\n",
        "           lst.append([sentences[i], sentences[j], score])\n",
        "        #print(lst)\n",
        "          \n",
        "        for ls in lst:\n",
        "          #print(ls[0])\n",
        "          #print(sentences[len(sentences)-1])\n",
        "          if ls[0] == sentences[len(sentences)-1] or ls[1] == sentences[len(sentences)-1]:\n",
        "            #print(sentences[len(sentences)-1])\n",
        "            #print(ls[0])\n",
        "            if  ls[2] > 0.65:\n",
        "                text_sim_obj = ContradictionFilter(self.context, self.reply)\n",
        "                text_sim = text_sim_obj.contradiction_detection()   #gives entailment, contradiction, neutral scores between context and response\n",
        "                return text_sim\n",
        "            else:\n",
        "              return 'Request Handover'\n",
        "          else:\n",
        "            continue        \n",
        "    \n",
        "\n",
        "end = time.time()    \n",
        "print(end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "0.00036597251892089844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qSAl6wnMvVO",
        "outputId": "d156e34b-ce3a-4641-d89c-7784e5a130ac"
      },
      "source": [
        "start = time.time()\n",
        "context = \"\"\"We are a finance company. \n",
        "             I like Python because I can build applications.\n",
        "             We are trying to give best service.\n",
        "             We do accept credit cards. \n",
        "             The cat walks on the sidewalk.\n",
        "             Credit cards are Capital One’s main physical product, and teams all over the company work to support this product whether it’s in the form of product development, software engineering, design, etc. For example, Alana is a data engineer working on rewards applications for external partners while Sahana works to support data scientists developing machine learning models used to derive insights from card customer calls. Like many others at Capital One, we know what needs to be done to support credit card initiatives, but we were curious about how physical credit cards actually work!\n",
        "             We compiled all of our research on EMV chip credit cards and shared it with our fellow Capital One associates during the company’s largest internal Software Engineering Conference (SECON). We were excited to see Software Engineers all over the company learning from our talk, which broke down EMV technology concepts into easily digestible components. Today, our Intro to EMV technology video is still the most-viewed and most-endorsed of all SECON 2020 videos.\n",
        "             Before we go further into EMV chip technology, let’s first talk about magnetic stripe technology. Magnetic stripes are a predecessor of EMV chips, so it’s helpful to understand how they work and some of the security concerns associated with these cards, which were later resolved by EMV chips.\n",
        "             In an EMV chip the access and manipulation of the physical chip components is a complex task that requires expensive high-tech equipment. Unlike the static data on a magnetic stripe, the data on an EMV chip is constantly changing which makes it difficult to isolate and extract. Additionally, EMV chips have the benefit of built-in encryption. When the chip card is dipped, it establishes an encrypted connection with the terminal to enforce the transaction rules written on the chip. Both the card and the terminal then collaborate to determine the outcome of the transaction.\n",
        "             When we started learning about EMV technology, we were fascinated by the complex, yet elegant design of the technology. Admittedly, the concepts were difficult for us to grasp at first, but as we continued our research and consulted internal subject matter experts about this technology, we were able to break down these concepts into understandable components that allowed us to solidify our understanding of Capital One’s main product. We hope that this breakdown of EMV chip technology helped provide insights into the fascinating technology powering 80.1% of worldwide credit card transactions today!\n",
        "             Also, magnetic stripe cards don’t have encryption. The card is simply a data store that is read by the payment terminal as-is. After the terminal reads the card, the terminal performs all the processing in collaboration with the issuer and/or payment system and the card is no longer involved in the rest of the process.\"\"\"\n",
        "cont = \"We accept credit card.\"\n",
        "reply = \"We accept cash.\" \n",
        "print(len(context.split(' ')))     \n",
        "text_infer_obj = ResponseFilter(context, reply)   \n",
        "text_infer = text_infer_obj.textual_similarity()   #gives entailment, contradiction, neutral scores between context and response\n",
        "print(text_infer)\n",
        "end = time.time()\n",
        "print('time taken:', end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "607\n",
            "Before we go further into EMV chip technology, let’s first talk about magnetic stripe technology. \t\t Magnetic stripes are a predecessor of EMV chips, so it’s helpful to understand how they work and some of the security concerns associated with these cards, which were later resolved by EMV chips. \t\t Score: 0.6808\n",
            "When we started learning about EMV technology, we were fascinated by the complex, yet elegant design of the technology. \t\t Admittedly, the concepts were difficult for us to grasp at first, but as we continued our research and consulted internal subject matter experts about this technology, we were able to break down these concepts into understandable components that allowed us to solidify our understanding of Capital One’s main product. \t\t Score: 0.6677\n",
            "Magnetic stripes are a predecessor of EMV chips, so it’s helpful to understand how they work and some of the security concerns associated with these cards, which were later resolved by EMV chips. \t\t Unlike the static data on a magnetic stripe, the data on an EMV chip is constantly changing which makes it difficult to isolate and extract. \t\t Score: 0.6460\n",
            "We compiled all of our research on EMV chip credit cards and shared it with our fellow Capital One associates during the company’s largest internal Software Engineering Conference (SECON). \t\t We were excited to see Software Engineers all over the company learning from our talk, which broke down EMV technology concepts into easily digestible components. \t\t Score: 0.6436\n",
            "In an EMV chip the access and manipulation of the physical chip components is a complex task that requires expensive high-tech equipment. \t\t When we started learning about EMV technology, we were fascinated by the complex, yet elegant design of the technology. \t\t Score: 0.6418\n",
            "Before we go further into EMV chip technology, let’s first talk about magnetic stripe technology. \t\t When we started learning about EMV technology, we were fascinated by the complex, yet elegant design of the technology. \t\t Score: 0.6273\n",
            "Credit cards are Capital One’s main physical product, and teams all over the company work to support this product whether it’s in the form of product development, software engineering, design, etc. \t\t Like many others at Capital One, we know what needs to be done to support credit card initiatives, but we were curious about how physical credit cards actually work! \t\t Score: 0.6266\n",
            "We were excited to see Software Engineers all over the company learning from our talk, which broke down EMV technology concepts into easily digestible components. \t\t Admittedly, the concepts were difficult for us to grasp at first, but as we continued our research and consulted internal subject matter experts about this technology, we were able to break down these concepts into understandable components that allowed us to solidify our understanding of Capital One’s main product. \t\t Score: 0.6260\n",
            "The card is simply a data store that is read by the payment terminal as-is. \t\t After the terminal reads the card, the terminal performs all the processing in collaboration with the issuer and/or payment system and the card is no longer involved in the rest of the process. \t\t Score: 0.6247\n",
            "When the chip card is dipped, it establishes an encrypted connection with the terminal to enforce the transaction rules written on the chip. \t\t The card is simply a data store that is read by the payment terminal as-is. \t\t Score: 0.6118\n",
            "None\n",
            "time taken: 0.15431499481201172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eywg9lKEy8pF",
        "outputId": "6be21a6d-9f51-4ff2-bfe9-4eb909ca35f9"
      },
      "source": [
        "import time\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "_do_print_debug_info = False\n",
        "\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "nlp =  spacy.load(\"en_core_web_md\")\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ContradictionFilter:\n",
        "    def __init__(self, context, reply):\n",
        "        self.context = context\n",
        "        self.reply = reply\n",
        "\n",
        "\n",
        "    def contradiction_detection(self):\n",
        "\n",
        "        sent1=self.context\n",
        "        sent2=self.reply\n",
        "        \n",
        "        doc1 = nlp(sent1)\n",
        "        doc2 = nlp(sent2)\n",
        "        \n",
        "        \n",
        "        #Antonyms finder function\n",
        "        synonyms = []\n",
        "        antonyms = []\n",
        "        def antysyn(word):\n",
        "            for syn in wordnet.synsets(word):\n",
        "                for l in syn.lemmas():\n",
        "                    synonyms.append(l.name())\n",
        "                    if l.antonyms():\n",
        "                        antonyms.append(l.antonyms()[0].name())  \n",
        "        \n",
        "        \n",
        "        #Initializing required variables and lists.\n",
        "        wrdlist=list()\n",
        "        antony=list()\n",
        "        contr_tracker=0\n",
        "        antonym_tracker=0\n",
        "        \n",
        "        negdoc1=0\n",
        "        negdoc2=0\n",
        "        verb1=\"\"\n",
        "        verb2=\"\"\n",
        "        num_contr_tracker=0\n",
        "        \n",
        "        \n",
        "        #Processing sentence 1\n",
        "        for token in doc1:\n",
        "            if(token.dep_==\"neg\"):\n",
        "                negdoc1=1\n",
        "                verb1+=\"NOT \"\n",
        "            #Storing the antonyms of root words\n",
        "            if(token.pos_==\"VERB\" and token.dep_==\"ROOT\"):\n",
        "                verb1+=token.lemma_\n",
        "                antysyn(token.lemma_)\n",
        "                for anton in antonyms:\n",
        "                    antony.append(anton)\n",
        "        \n",
        "        \n",
        "        #Processing Sentence 2\n",
        "        for token in doc2:\n",
        "            if(token.dep_==\"neg\"):\n",
        "                negdoc2=1\n",
        "                verb2+=\"NOT \"\n",
        "            if(token.pos_==\"VERB\" and token.dep_==\"ROOT\"):\n",
        "                verb2+=token.lemma_\n",
        "                if(token.lemma_ in antony):\n",
        "                    antonym_tracker=1\n",
        "        \n",
        "        \n",
        "        \n",
        "        #Function for checking negation\n",
        "        def checknegationcontradiction(antonym_tracker,negdoc1,negdoc2):\n",
        "            temp_var=negdoc1+negdoc2+antonym_tracker\n",
        "            if(temp_var%2!=0 and temp_var<3):\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "        \n",
        "        #Checking contradiction due to negation\n",
        "        contr_tracker=checknegationcontradiction(antonym_tracker,negdoc1,negdoc2)\n",
        "        \n",
        "        \n",
        "        def check_words(doc):\n",
        "            merged_word=\"\"\n",
        "            WordNum  = len(doc)\n",
        "            for i in range(WordNum):\n",
        "                if(doc[i].ent_type_==\"CARDINAL\" or doc[i].pos_==\"NUM\"):\n",
        "                    merged_word = doc[i].text\n",
        "                if((doc[i].ent_type_==\"CARDINAL\" or doc[i].pos_==\"NUM\") and str.isdigit(doc[i].text)):\n",
        "                    if(not(str.isdigit(doc[i-1].text))):\n",
        "                        if(not(str.isdigit(doc[i-2].text))):\n",
        "                            merged_word= doc[i-2].text+' '+doc[i-1].text+' '+doc[i].text\n",
        "            return merged_word\n",
        "        \n",
        "        \n",
        "        def check_values(t1,t2):\n",
        "            for phrase in checklist_more:\n",
        "                if(t1.find(phrase)!=-1 and t2.find(phrase)==-1):\n",
        "                    num1 = t1.replace(phrase,'')\n",
        "                    num2 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "                    num2=num2[0]\n",
        "                    if int(num1)>num2:\n",
        "                        print(\"case1\")\n",
        "                        return('Contradiction')\n",
        "                    else:\n",
        "                        return(\"No Contradiction\")\n",
        "                else:\n",
        "                    return(\"No Contradiction\")\n",
        "            for phrase in checklist_more:\n",
        "                if(t2.find(phrase)!=-1 and t1.find(phrase)==-1):\n",
        "                    num2 = t2.replace(phrase,'')\n",
        "                    num1 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "                    num1=num1[0]\n",
        "                    if int(num2)>num1:\n",
        "                        print(\"case2\")\n",
        "                        return('Contradiction') \n",
        "                    else:\n",
        "                        return(\"No Contradiction\")\n",
        "                else:\n",
        "                    return(\"No Contradiction\")\n",
        "            for phrase in checklist_less:\n",
        "                if(t1.find(phrase)!=-1 and t2.find(phrase)==-1):\n",
        "                    num1 = t1.replace(phrase,'')\n",
        "                    num2 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "                    num2=num2[0]\n",
        "                    if int(num1)<num2:\n",
        "                        print(\"case3\")\n",
        "                        return('Contradiction')\n",
        "                    else:\n",
        "                        return(\"No Contradiction\")\n",
        "                else:\n",
        "                        return(\"No Contradiction\")\n",
        "            for phrase in checklist_less:\n",
        "                if(t2.find(phrase)!=-1 and t1.find(phrase)==-1):\n",
        "                    num2 = t2.replace(phrase,'')\n",
        "                    num1 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
        "                    num1=num1[0]\n",
        "                    if int(num1)>num2:\n",
        "                        print(\"case4\")\n",
        "                        return('Contradiction')\n",
        "                    else: \n",
        "                        return(\"No Contradiction\")\n",
        "                else:\n",
        "                        return(\"No Contradiction\")\n",
        "            else:\n",
        "                if(t1!=t2):\n",
        "                    return('Contradiction') \n",
        "        \n",
        "        #Finding numerical mismatch\n",
        "        checklist_more=['more than ', 'greater than ', 'above', 'all']\n",
        "        checklist_less=['less than ', 'lesser than ', 'below', 'few']\n",
        "        \n",
        "        \n",
        "        x1 = check_words(doc1)\n",
        "        y1 = check_words(doc2)\n",
        "        number_contr_tracker=check_values(x1,y1)\n",
        "        if(number_contr_tracker=='Contradiction'):\n",
        "            num_contr_tracker=1\n",
        "        else:\n",
        "            num_contr_tracker=0\n",
        "        \n",
        "        if contr_tracker==0 and num_contr_tracker==0:\n",
        "            return self.reply\n",
        "        \n",
        "        if contr_tracker==1 or num_contr_tracker==1:\n",
        "            return 'Request Handover'\n",
        "\n",
        "\n",
        "\n",
        "class ResponseFilter:\n",
        "    def __init__(self, context, reply):\n",
        "        self.context = context\n",
        "        self.reply = reply\n",
        "\n",
        "    def textual_similarity(self):\n",
        "         # Single list of sentences - Possible tens of thousands of sentences\n",
        "        context_reply = (self.context + ' ' + self.reply)\n",
        "        sent_list = nltk.tokenize.sent_tokenize(context_reply)\n",
        "        sentences = sent_list\n",
        "        first_half_context, second_half_context = self.context[:len(self.context)//2], self.context[len(self.context)//2:]\n",
        "\n",
        "        paraphrases = util.paraphrase_mining(model, sentences)\n",
        "        lst = []\n",
        "\n",
        "        for paraphrase in paraphrases[0:100]:\n",
        "           score, i, j = paraphrase\n",
        "           lst.append([sentences[i], sentences[j], score])\n",
        "          \n",
        "        for ls in lst:\n",
        "          print('sim score :   ', ls[2])\n",
        "          if ls[0] == sentences[len(sentences)-1]:\n",
        "            if 0.75 < ls[2] < 0.95:\n",
        "                  text_sim_obj = ContradictionFilter(ls[1], self.reply)\n",
        "                  text_sim = text_sim_obj.contradiction_detection()   \n",
        "                  return text_sim       \n",
        "            elif ls[2]>=0.95:\n",
        "                  return self.reply \n",
        "            else:\n",
        "                  return 'Request Handover'\n",
        "\n",
        "          elif ls[1] == sentences[len(sentences)-1]:\n",
        "            if 0.75 < ls[2] < 0.95:\n",
        "                  text_sim_obj = ContradictionFilter(ls[1], self.reply)\n",
        "                  text_sim = text_sim_obj.contradiction_detection() \n",
        "                  return text_sim          \n",
        "            elif ls[2]>=0.95:\n",
        "                  return self.reply \n",
        "            else:\n",
        "                  return 'Request Handover'         \n",
        "          else:\n",
        "            continue  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldPRFEOzzVzN",
        "outputId": "867575b6-9e79-4868-eb65-5243f8a50e15"
      },
      "source": [
        "start = time.time()\n",
        "context = \"\"\"We are a finance company. \n",
        "             I like Python because I can build applications.\n",
        "             We are trying to give best service.\n",
        "             We do accept credit cards. \n",
        "             The cat walks on the sidewalk.\n",
        "             A credit card is a payment card issued to users (cardholders) to enable the cardholder to pay a merchant for goods and services based on the cardholder's accrued debt (i.e., promise to the card issuer to pay them for the amounts plus the other agreed charges). The card issuer (usually a bank) creates a revolving account and grants a line of credit to the cardholder, from which the cardholder can borrow money for payment to a merchant or as a cash advance.\n",
        "             This sometimes led to a case of mistaken identity, either accidentally or intentionally, by acting on behalf of the charge account owner or out of malice to defraud both the charge account owner and the merchant. Beginning in the 1930s, merchants started to move from charge coins to the newer Charga-Plate. Credit card numbers were originally embossed to allow easy transfer of the number to charge slips.\n",
        "             Credit cards are Capital One’s main physical product, and teams all over the company work to support this product whether it’s in the form of product development, software engineering, design, etc. For example, Alana is a data engineer working on rewards applications for external partners while Sahana works to support data scientists developing machine learning models used to derive insights from card customer calls. Like many others at Capital One, we know what needs to be done to support credit card initiatives, but we were curious about how physical credit cards actually work!\n",
        "             We compiled all of our research on EMV chip credit cards and shared it with our fellow Capital One associates during the company’s largest internal Software Engineering Conference (SECON). We were excited to see Software Engineers all over the company learning from our talk, which broke down EMV technology concepts into easily digestible components. Today, our Intro to EMV technology video is still the most-viewed and most-endorsed of all SECON 2020 videos.\n",
        "             Before we go further into EMV chip technology, let’s first talk about magnetic stripe technology. Magnetic stripes are a predecessor of EMV chips, so it’s helpful to understand how they work and some of the security concerns associated with these cards, which were later resolved by EMV chips.\n",
        "             In an EMV chip the access and manipulation of the physical chip components is a complex task that requires expensive high-tech equipment. Unlike the static data on a magnetic stripe, the data on an EMV chip is constantly changing which makes it difficult to isolate and extract. Additionally, EMV chips have the benefit of built-in encryption. When the chip card is dipped, it establishes an encrypted connection with the terminal to enforce the transaction rules written on the chip. Both the card and the terminal then collaborate to determine the outcome of the transaction.\n",
        "             When we started learning about EMV technology, we were fascinated by the complex, yet elegant design of the technology. Admittedly, the concepts were difficult for us to grasp at first, but as we continued our research and consulted internal subject matter experts about this technology, we were able to break down these concepts into understandable components that allowed us to solidify our understanding of Capital One’s main product. We hope that this breakdown of EMV chip technology helped provide insights into the fascinating technology powering 80.1% of worldwide credit card transactions today!\n",
        "             Also, magnetic stripe cards don’t have encryption. The card is simply a data store that is read by the payment terminal as-is. After the terminal reads the card, the terminal performs all the processing in collaboration with the issuer and/or payment system and the card is no longer involved in the rest of the process.\"\"\"\n",
        "cont = \"We accept credit card.\"\n",
        "reply = \"We accept credit cards and cash.\" \n",
        "print(len(context.split(' ')))        \n",
        "text_infer_obj = ResponseFilter(context, reply)\n",
        "text_infer = text_infer_obj.textual_similarity()   #gives entailment, contradiction, neutral scores between context and response\n",
        "print(text_infer)\n",
        "end = time.time()\n",
        "print('time taken:', end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "780\n",
            "sim score :    0.8328469395637512\n",
            "We accept credit cards and cash.\n",
            "time taken: 0.10916733741760254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YTBpUov3BV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61b8fcb-18c8-41f4-c353-6610c6cdff0e"
      },
      "source": [
        "# Python3 code to demonstrate working of\n",
        "# Splitting string into equal halves\n",
        "# Using string slicing\n",
        "\n",
        "# initializing string\n",
        "test_str = \"Geeks for Geeks\"\n",
        "\n",
        "# printing original string\n",
        "print(\"The original string is : \" + test_str)\n",
        "\n",
        "# Using string slicing\n",
        "# Splitting string into equal halves\n",
        "res_first, res_second = test_str[:len(test_str)//2],test_str[len(test_str)//2:]\n",
        "\n",
        "# printing result\n",
        "print(\"The first part of string : \" + res_first)\n",
        "print(\"The second part of string : \" + res_second)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The original string is : Geeks for Geeks\n",
            "The first part of string : Geeks f\n",
            "The second part of string : or Geeks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ep38Xrm6yyI",
        "outputId": "fb8aab50-a453-4920-9292-fe48240a8c32"
      },
      "source": [
        "str = 'The second part of string'\n",
        "print(str.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'second', 'part', 'of', 'string']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_7pVMm13hAA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1VujcA73hI3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poXnFwcH3hLt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvcFoKl23hOW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNxkSJrfKNWd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}